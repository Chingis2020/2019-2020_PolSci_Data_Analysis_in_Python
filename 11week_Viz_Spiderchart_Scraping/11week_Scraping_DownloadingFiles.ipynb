{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XXbvlWGuFQgL"
   },
   "source": [
    "# Scraping 101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TXTlYgvASMRO"
   },
   "source": [
    "# Kittens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 251
    },
    "colab_type": "code",
    "id": "mMiuZCumSRpn",
    "outputId": "ba2e6570-cc56-4838-cc1e-5051b0f51a40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      __     __,\n",
      "                      \\,`~\"~` /\n",
      "      .-=-.           /    . .\\\n",
      "     / .-. \\          {  =    Y}=\n",
      "    (_/   \\ \\          \\      / \n",
      "           \\ \\        _/`'`'`b\n",
      "            \\ `.__.-'`        \\-._\n",
      "             |            '.__ `'-;_\n",
      "             |            _.' `'-.__)\n",
      "              \\    ;_..-`'/     //  \\\n",
      "              |   /  /   |     //    |\n",
      "              \\  \\ \\__)   \\   //    /\n",
      "               \\__)  \n"
     ]
    }
   ],
   "source": [
    "import requests \n",
    "\n",
    "try:\n",
    "    request = requests.get(\"http://placekitten.com/\")\n",
    "    kittens = request.text\n",
    "    print (kittens[559:1000])\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print (\"No kittenz. Got an error code:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HaaopewSwxX"
   },
   "source": [
    "# PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n_n6MmAPBOKi"
   },
   "source": [
    "## Download pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IdJhH8WmVG9l"
   },
   "source": [
    "Импортируем модуль requests и откроем файл для записи, в который потом сохраним нашу pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iKnBixMMl7g4"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "o = open(\"press.pdf\",'wb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j5mWzs8cl-yy"
   },
   "source": [
    "Мы будем сохранять отчет Евро Парламента. У нас уже есть ссылка на саму pdf, поэтому просто укажем ее. С помощью модуля requests откроем ссылку для потокового скачивания файла (размер pdf может быть большим и не факт что он уместиться за раз в памяти)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "25YaHRRYSz7-"
   },
   "outputs": [],
   "source": [
    "url=(\"http://www.europarl.europa.eu/pdfs/news/public/story/20161026STO49002/20161026STO49002_en.pdf\" )\n",
    "release=requests.get(url, stream=True)\n",
    "press_release=release.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XUAr7NuUm0pB"
   },
   "source": [
    "Осталось все записать файл и закрыть его запись"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VqVItzowm1PO"
   },
   "outputs": [],
   "source": [
    "o.write(press_release)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CVBXpJRNBSlh"
   },
   "source": [
    "## Download all PDFs from page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fg8QewuBnF42"
   },
   "source": [
    "Попробуем задание посложнее. Скачаем **все** pdf, которые есть на странице с курсами ВШЭ для направления политология. Зайдя на сайт и выбрав необходимые фильтры, получаем следующую страницы, на которой есть инормация о курсах:\n",
    "\n",
    "https://www.hse.ru/edu/courses/index.html?words=&full_words=&edu_year=2018&lecturer=&edu_level=78397&language=&level=1191462%3A130671177&mandatory=&is_dpo=0&filial=22723&modules=10162246&modules=10162247&xlc=&genelective=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydhfoA3pnmPd"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "url = (\"https://www.hse.ru/edu/courses/index.html?words=&full_words=&edu_year=2018&lecturer=&edu_level=78397&language=&level=1191462%3A130671177&mandatory=&is_dpo=0&filial=22723&modules=10162246&modules=10162247&xlc=&genelective=0\")\n",
    "page = requests.get(url)\n",
    "courses = page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cLl2nKZLu-Rq"
   },
   "source": [
    "Используем модуль bs4, а конкретно метод BeautifulSoup, который представит нам сайт в удобном для работы виде (информация будет разбита по тэгам веб страницы div, a и т.д.)\n",
    "\n",
    "В списке links будет хранить ссылки на наши pdf файлы с курсами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HcXMZUAFvcsC"
   },
   "outputs": [],
   "source": [
    "links = [] \n",
    "\n",
    "soup = bs4.BeautifulSoup(courses, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "M8d5Hd0Hvp74"
   },
   "source": [
    "Изучив код страницы, мы видим что ссылки на pdf лежат в теге \"а\". Поэтому найдем все такие теги и заберем адрес ссылки, которая хранится внутри тега \"а\" с параметром href. Проверим, ссылка ли это на курс (в этом теге могут быть ссылки на элементы меню) и есть ли такая ссылка в нашем списке, и если нет, то добавим ее.\n",
    "\n",
    "**Всегда** придется изучать строение самой таблицы, чтобы обращаться и брать ее элементы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gajz51MEwMdl"
   },
   "outputs": [],
   "source": [
    "for name in soup.findAll (\"a\"):\n",
    "    course = name.get (\"href\")\n",
    "    if \"hse.ru/data/\" in str(course):\n",
    "        if str(course) not in links:\n",
    "            links.append(course)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.hse.ru/data/2019/09/03/1491358191/program-2855895948-0wXzMk9PI5.pdf',\n",
       " 'https://www.hse.ru/data/2019/10/07/1536829037/program-3545935072-ANrx0EiOCZ.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/09/1491361404/program-2844123818-oQud3m8EjA.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/09/1491361376/program-2874011060-IlSg71As3V.pdf',\n",
       " 'https://www.hse.ru/data/2019/07/03/1491340575/program-2856872089-k8pvU_GvCU.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/12/1491598376/program-2829284138-RKNJlnkM1d.pdf',\n",
       " 'https://www.hse.ru/data/2019/10/14/1491597605/program-2874731637-Tg4F0JMfUU.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/15/1491597601/program-2883828071-16cqW_EkxJ.pdf',\n",
       " 'https://www.hse.ru/data/2019/07/03/1491341093/program-2880421577-z1Iwqcp4SM.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/05/1491341097/program-2874707073-REi4Mn2MR8.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/13/1491338598/program-2859717269-TKeLNuPKR3.pdf',\n",
       " 'https://www.hse.ru/data/2019/11/10/1531702074/program-2853069879-sMJoRTH1zM.pdf',\n",
       " 'https://www.hse.ru/data/2019/09/20/1491350780/program-2851358617-G_AA7w9pG_.pdf']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m91hfl6dxFCF"
   },
   "source": [
    "Получили список ссылок на pdf со **всей** страницы.\n",
    "\n",
    "Теперь мы пройдемся по всем этим ссылкам и точно также как при скачивании pdf с сайта Евро Парламента просто откроем новым requests'ом и запишем в файл. Название файла будет точно таким же как он назывался на сайте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-L1tDTGUAEC"
   },
   "outputs": [],
   "source": [
    "for link in links: \n",
    "    course = requests.get(link, stream=True)\n",
    "    course_content = course.content\n",
    "    o = open(\"\" + link[link.rfind(\"/\") + 1:], \"wb\") # link[link.rfind(\"/\") + 1:] этим мы берем название файла из конца ссылки\n",
    "    o.write(course_content)\n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BL73pdB4BZ4Y"
   },
   "source": [
    "## Download all PDFs from several pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNg7AD3uxcru"
   },
   "source": [
    "Еще усложним задачу. Теперь мы будем собирать pdf с нескольких страницы выдачи. Код будет похож на прошлый пример и мы только обернем его циклом по страницам.\n",
    "\n",
    "Заведем переменную *page* для хранения текущей страницы и будем увеличивать ее после завершения прохода текущей страницы.\n",
    "\n",
    "Для выбора новой страницы просто подняем цифру в адресе сайт на нужную. Пример:\n",
    "https://www.hse.ru/edu/courses/page2.html...\n",
    "https://www.hse.ru/edu/courses/page3.html...\n",
    "https://www.hse.ru/edu/courses/page4.html...\n",
    "\n",
    "И т.д.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q3DYKGG5We9-"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "\n",
    "page = 1\n",
    "while page <= 3: #количество страниц, по которым нужно пройти\n",
    "    \n",
    "    #делаем запрос к нужной странице\n",
    "    url = (\"https://www.hse.ru/edu/courses/page\" + str(page) + \".html?words=&full_words=&edu_year=2018&lecturer=&edu_level=78397&language=&level=1191462%3A130671177&mandatory=&is_dpo=0&filial=22723&modules=10162246&modules=10162247&xlc=&genelective=0\")\n",
    "    pages = requests.get(url)\n",
    "    courses = pages.text\n",
    "    \n",
    "    #дальше код полностью с прошлого примера\n",
    "\n",
    "    links = []  \n",
    "\n",
    "    soup = bs4.BeautifulSoup(courses, \"html.parser\")\n",
    "\n",
    "    for name in soup.findAll (\"a\"):\n",
    "        course = name.get (\"href\")\n",
    "        if \"hse.ru/data/\" in str(course):\n",
    "            if str(course) not in links:\n",
    "                links.append(course)\n",
    "                \n",
    "    for link in links: \n",
    "        course = requests.get(link, stream=True)\n",
    "        course_content = course.content\n",
    "        o = open(\"\" + link[link.rfind(\"/\") + 1:], \"wb\") # link[link.rfind(\"/\") + 1:] этим мы берем название файла из конца ссылки\n",
    "        o.write(course_content)\n",
    "            \n",
    "    page += 1 # \"берем\" новую страницу\n",
    "    \n",
    "o.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Duma\n",
    "\n",
    "В этом примере мы зайдем на сайт государственной думы и попробуем получить данные обо всех ее депутатах, в какой фракции и комитете он состоит, а также занимаю должность. Все это в итоге представим в виде DataFrame и сохраним как csv файл, для дальнейшей работы.\n",
    "\n",
    "Делаем все как обычно, заходим на сайт думы, используем requests и BeatifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "url = \"http://duma.gov.ru/duma/deputies/7/\"\n",
    "page = requests.get(url)\n",
    "\n",
    "text = page.text\n",
    "soup = BeautifulSoup(text, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имена депутатов будет хранить в deputates, комитеты в которых они состоят в commissions, должность в occupations и партию в party."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "deputates = []\n",
    "commissions = []\n",
    "occupations = []\n",
    "party = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Депутатов много (487 человек) и скрейпинг может занимать прилично времени. Поэтому ограничим количество обработанных человек с помощью счетчика count и проверки. Для примера оставим 10 человек.\n",
    "\n",
    "Данные обо всхе депутатах находятся в теге \"a\" и с классом \"person__image-wrapper person__image-wrapper--s\"\n",
    "\n",
    "Найдем все такие теги"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0    \n",
    "person = soup.find_all('a', class_ = \"person__image-wrapper person__image-wrapper--s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь для каждого депутата мы должны получить его имя и ссылку, по которой мы пройдем и выясним остальную информацию: комитет, должность и фракцию.\n",
    "\n",
    "К нашему счастью вместе с фото депутата в теге \"img\" хранится элемент  \"alt\" которой по наведению на фото, покажет в левом нижнем углу текст, который в нашем случае представляет собой Имя Фамилию и Отчество. Если бы не это, нам пришлось бы делать два поиска по тегам, т.к. имя хранится в одном теге, а фамилия и отчество в другом.\n",
    "\n",
    "В том же теге \"img\" хранится ссылка на страницу с этим конкретным депутатом. Получим ее из элемента \"href\". Поэтому не нужно делать новый поиск тега.\n",
    "\n",
    "Делаем обычный request к странице депутата, как мы делали в примере с PDF.\n",
    "\n",
    "На странице с депутатом нам нужно найти тег в котором хранится его комитет и фракция. Посмотрев исходный код страницы, находим нужные теги - \"a\" с классами link \"link--underline link--external\" и \"link link--underline person__description__link\". Сохраняем их по своим массивам\n",
    "\n",
    "Одна проблема, должность в комитете находится в строке с самим комитетом. Но к нашему счастью, они указываются в скобочках. Поэтому найдем индекс первого появления скобочек в строке с комитетом. Вырежем их и запишем в нужный массив. Точно также запишем комитет, обрезав у него конец строки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a class=\"person__image-wrapper person__image-wrapper--s\" href=\"/duma/persons/99112936/\" itemprop=\"url\">\n",
       "<picture>\n",
       "<img alt=\"Авдеев Александр Александрович\" class=\"person__image person__image--s\" itemprop=\"image\" src=\"/media/persons/240x240/OM9mmw6lAZ1lhnpM1yYBIMAo1BaQgxnz.jpg\"/>\n",
       "</picture>\n",
       "</a>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "person[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<img alt=\"Авдеев Александр Александрович\" class=\"person__image person__image--s\" itemprop=\"image\" src=\"/media/persons/240x240/OM9mmw6lAZ1lhnpM1yYBIMAo1BaQgxnz.jpg\"/>\n",
      "Авдеев Александр Александрович\n",
      "/duma/persons/99112936/\n",
      "[]\n",
      "http://duma.gov.ru//duma/persons/99112936/\n",
      "Комитет по контролю и Регламенту \n"
     ]
    }
   ],
   "source": [
    "for p in person:\n",
    "    print(p.find('img'))\n",
    "    name = p.find('img')\n",
    "    print(name.get('alt'))\n",
    "    print(p.get('href'))\n",
    "    print(deputates)\n",
    "    url_deput = \"http://duma.gov.ru/\" + p.get(\"href\")\n",
    "    print(url_deput)\n",
    "    page = requests.get(url_deput)\n",
    "    soup_deput = BeautifulSoup(page.text, \"html.parser\")\n",
    "    comm = soup_deput.find('a', class_ = \"link link--underline link--external\")\n",
    "    #print(comm)\n",
    "    t = comm.text.strip()\n",
    "    t_new = t[:t.find('(')]\n",
    "    print(t_new)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'('"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = comm.text.strip()\n",
    "t[t.find('(')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in person:  \n",
    "    \n",
    "    # Обрезаем скрейпинг, т.к. очень долго идет обработка\n",
    "    if count > 10:\n",
    "        break\n",
    "    \n",
    "    #Получаем имена\n",
    "    name = p.find('img', class_ = 'person__image person__image--s') # находим тег img и забираем из него значение alt\n",
    "    deputates.append(name.get(\"alt\"))\n",
    "    \n",
    "    # Получаем ссылку на депутата\n",
    "    url_deput = \"http://duma.gov.ru/\" + p.get(\"href\") # в нашем найденном теге уже есть нужная ссылка на депутата. Забираем ее\n",
    "    \n",
    "    # Делаем запрос\n",
    "    page = requests.get(url_deput)\n",
    "    soup_deput = BeautifulSoup(page.text, \"html.parser\")\n",
    "    \n",
    "    #Ищем комитет\n",
    "    comm = soup_deput.find('a', class_ = \"link link--underline link--external\")\n",
    "    t = comm.text.strip(\"\\n\")\n",
    "    # регулярным выражением убираем все непечатный символы\n",
    "    commissions.append(re.sub('\\W+',' ',t[:t.index(\"(\")-1]).strip()) # убираем должность\n",
    "    occupations.append(t[t.index(\"(\") + 1:t.index(\")\")]) #должность всегда в скобочках, \n",
    "                                                         #находим первые вхождени ( и )\n",
    "\n",
    "    # Поиск фракции\n",
    "    part = soup_deput.find('a', class_ = \"link link--underline person__description__link\")\n",
    "    party.append(part.text)\n",
    "\n",
    "    count += 1 # обновляем счетчик"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все данные получены, осталось создать DataFrame и сохранить его в виде csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'Депутат': deputates, 'Коммитет':commissions ,'Должность':occupations, 'Фракция': party})\n",
    "df.to_csv(\"Gos_duma.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно посмотреть на первые несколького элементов используя команду df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Депутат</th>\n",
       "      <th>Коммитет</th>\n",
       "      <th>Должность</th>\n",
       "      <th>Фракция</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Авдеев Александр Александрович</td>\n",
       "      <td>Комитет по контролю и Регламенту</td>\n",
       "      <td>член комитета</td>\n",
       "      <td>«ЕДИНАЯ РОССИЯ»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Авдеев Михаил Юрьевич</td>\n",
       "      <td>Комитет по транспорту и строительству</td>\n",
       "      <td>Первый заместитель председателя комитета</td>\n",
       "      <td>КПРФ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Агаев Ваха Абуевич</td>\n",
       "      <td>Комитет по финансовому рынку</td>\n",
       "      <td>заместитель председателя комитета</td>\n",
       "      <td>КПРФ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Адучиев Батор Канурович</td>\n",
       "      <td>Комитет по аграрным вопросам</td>\n",
       "      <td>член комитета</td>\n",
       "      <td>«ЕДИНАЯ РОССИЯ»</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Азимов Рахим Азизбоевич</td>\n",
       "      <td>Комитет по безопасности и противодействию корр...</td>\n",
       "      <td>член комитета</td>\n",
       "      <td>«ЕДИНАЯ РОССИЯ»</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Депутат  \\\n",
       "0  Авдеев Александр Александрович   \n",
       "1           Авдеев Михаил Юрьевич   \n",
       "2              Агаев Ваха Абуевич   \n",
       "3         Адучиев Батор Канурович   \n",
       "4         Азимов Рахим Азизбоевич   \n",
       "\n",
       "                                            Коммитет  \\\n",
       "0                   Комитет по контролю и Регламенту   \n",
       "1              Комитет по транспорту и строительству   \n",
       "2                       Комитет по финансовому рынку   \n",
       "3                       Комитет по аграрным вопросам   \n",
       "4  Комитет по безопасности и противодействию корр...   \n",
       "\n",
       "                                  Должность          Фракция  \n",
       "0                             член комитета  «ЕДИНАЯ РОССИЯ»  \n",
       "1  Первый заместитель председателя комитета             КПРФ  \n",
       "2         заместитель председателя комитета             КПРФ  \n",
       "3                             член комитета  «ЕДИНАЯ РОССИЯ»  \n",
       "4                             член комитета  «ЕДИНАЯ РОССИЯ»  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "TXTlYgvASMRO",
    "2MZR16BcFQie",
    "8HaaopewSwxX",
    "gmwbXFgtzfLa",
    "pC8PEFYMxrhI",
    "ZxExEaxdZRLV"
   ],
   "name": "Scraping101.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
